+++
title = "SPSC ç¯å½¢é˜Ÿåˆ—ä¼˜åŒ–"
date = "2024-12-26"

[taxonomies]
tags = ["entry"]
+++

# Cache

æ¥æº <https://www.reddit.com/r/rust/comments/1h3bqv0/why_is_ringbuf_crate_so_fast/>ï¼Œ

> 5x sounds like about the speed up you get from having cached indices, where the reader caches the write head and the writer caches the read tail, and they only update those to the â€œrealâ€ atomic values of the other when they run out of items or space, respectively.

> No I donâ€™t mean simply padding out the cache line, I mean there are two copies of the read index and two copies of the write index, one of each is the atomic that they write to, and the other is non-atomic which the opposite thread reads from

é‡ç‚¹åœ¨äºå‡å°‘äº† atomic çš„ä½¿ç”¨ã€‚

å› ä¸ºåœ¨ SPSC æ¨¡å¼ä¸‹ï¼ŒWriter è¯»å–çš„ `head` æŒ‡é’ˆåªæ˜¯ç”¨æ¥åˆ¤æ–­é˜Ÿåˆ—æ˜¯å¦å·²æ»¡ï¼Œå› æ­¤è¿™ä¸ªæŒ‡é’ˆå®Œå…¨å¯ä»¥è¢«æ‹¿æ¥ç¼“å­˜ï¼Œè¿™æ ·åœ¨å¤§é‡æ•°æ®çš„æƒ…å†µä¸‹éå¸¸æœ‰ç”¨ã€‚Reader åŒç†ï¼Œå¯ä»¥ç¼“å­˜ `tail` æŒ‡é’ˆæ¥åˆ¤æ–­æ˜¯å¦ä¸ºç©ºï¼Œä»…å½“åˆ¤æ–­åˆ°é˜Ÿåˆ—ä¸ºç©ºæ—¶æ‰ä¼šæ‰§è¡Œä¸€æ¬¡ atomic æ¥æ›´æ–°å¯¹åº”çš„æŒ‡é’ˆã€‚

å¦å¤–è¿˜éœ€è¦è€ƒè™‘ cache çš„ false sharing é—®é¢˜ï¼Œæ‰€ä»¥æœ€å¥½æ¯ä¸ª atomic pointer éƒ½æœ€å¥½ padding ä¸€ä¸‹ã€‚

åˆå­¦åˆ°äº†æ²¡å•¥ç”¨çš„çŸ¥è¯†ï¼Œå¥½è€¶ğŸ˜†ï¼

In detail: [https://rigtorp.se/ringbuffer](https://rigtorp.se/ringbuffer/)

# å°è¯•ç†è§£

å…¶å®æ— é”è¿™ä¸ªä¸œè¥¿ï¼ˆç‰¹æŒ‡ Atomicï¼‰è¦æƒ³æ·±å…¥è¿˜æ˜¯å¾ˆå¤æ‚çš„ï¼Œä¸»è¦æ˜¯è¦è€ƒè™‘åˆ°å†…å­˜åºçš„é—®é¢˜ï¼Œè¿™è™½ç„¶ä¹Ÿæ˜¯ä¸ªè€ç”Ÿå¸¸è°ˆçš„è¯é¢˜äº†ã€‚

å…¶å®æ€»çš„æ¥çœ‹ï¼Œå†…å­˜åºï¼ˆ**åªé’ˆå¯¹å•ä¸ªæ ¸å¿ƒ**ï¼‰ä¸»è¦å°±æ˜¯ load store çš„å››ç§æ’åˆ—ç»„åˆï¼Œ

* Load-Loadï¼Œä¸¤ä¸ª Load æŒ‡ä»¤éœ€è¦è¢«åŒæ­¥
* Load-Storeï¼Œå‰é¢çš„ Load æŒ‡ä»¤ä¸åé¢çš„ Store æŒ‡ä»¤éœ€è¦è¢«åŒæ­¥
* Store-Storeï¼Œä¸¤ä¸ª Store æŒ‡ä»¤éœ€è¦è¢«åŒæ­¥
* Store-Loadï¼Œå‰é¢çš„ Store æŒ‡ä»¤ä¸åé¢çš„ Load æŒ‡ä»¤éœ€è¦è¢«åŒæ­¥

è¿™é‡Œçš„æŒ‡ä»¤ç²’åº¦éƒ½åªæ˜¯æŒ‡ä»¤æœ¬èº«ï¼Œè·Ÿæ“ä½œçš„å†…å­˜åŒºåŸŸæ— å…³ã€‚ä¾‹å¦‚ ARM64ï¼Œåªè¦ä½ æ˜¯æŒ‰ç…§å­—èŠ‚å¯¹é½çš„æ–¹å¼ï¼ˆè®¿é—®å¤šå°‘å­—èŠ‚å°±éœ€è¦å¤šå°‘å­—èŠ‚å¯¹é½ï¼Œä¾‹å¦‚è®¿é—®ä¸€ä¸ª `u64` å˜é‡å°±éœ€è¦æ˜¯ 8 å­—èŠ‚å¯¹é½çš„ï¼‰ï¼Œé‚£ä¹ˆéƒ½èƒ½ä¿è¯æŒ‡ä»¤çš„åŸå­æ€§ï¼ˆæ‰§è¡Œå®ŒæŒ‡ä»¤åå…¶ä»– CPU çœ‹åˆ°çš„åœ°å€å†…å®¹ä¸€å®šæ˜¯æœ€æ–°çš„ï¼‰ã€‚

<https://davekilian.com/acquire-release.html>

<https://en.wikipedia.org/wiki/Load-Hit-Store>

# æ€ä¹ˆé€‰æ‹©

å…¶å®è¯´äº†è¿™ä¹ˆå¤šåŸç†ï¼Œæœ€é‡è¦çš„è¿˜æ˜¯è¦å†³å®šé€‰æ‹©å“ªç§å†…å­˜åºï¼Œéå¸¸æ¨è [Preshing](https://preshing.com/20120913/acquire-and-release-semantics/) çš„è¿™ç¯‡æ–‡ç« ï¼Œé€šå¸¸æ¥è¯´é€‰æ‹©ä¸€èˆ¬å°±æ˜¯ä¸‰ç§ï¼Œ

* Relaxedï¼Œä¸å¸¦ä»»ä½•çº¦æŸï¼Œåªä¿è¯åŸå­æ€§
* Acquire
* Release
* Sequenceï¼Œä¸æ˜¯å¾ˆå¸¸ç”¨ï¼ŒåŸºæœ¬ä¸Šåªæœ‰åœ¨ä¸ç¡®å®šçš„æ—¶å€™æ‰é€‰æ‹©

 ![From Preshing](/api/attachments.redirect?id=e042ec5b-a0b3-4c48-b777-33f38ccf3ec1)

è¿™ä¸ªå›¾å…¶å®å°±èƒ½çœ‹çš„æ¯”è¾ƒæ¸…æ™°äº†ï¼š

å¯¹äº Acquireï¼Œå§‹ç»ˆéœ€è¦ä¿è¯ä¹‹å‰çš„ Load æ“ä½œå®Œæˆï¼Œç„¶åæ‰èƒ½æ‰§è¡Œåç»­çš„ Load æˆ–è€… Storeã€‚

å¯¹äº Releaseï¼Œå§‹ç»ˆéœ€è¦ä¿è¯ Store ä¹‹å‰çš„ Load æˆ–è€… Store æ“ä½œå®Œæˆï¼Œç„¶åæ‰èƒ½æ‰§è¡Œ Storeã€‚

ä¸»è¦æ˜¯æä¾› locking ç›¸å…³çš„åŠŸèƒ½çš„ï¼Œä¾‹å¦‚ spinlock è¿™ä¸ªæ“ä½œï¼Œ

```c
spin_lock(&lock);
global_vars_a *= 2;
spin_unlock(&lock);
```

æ¯”è¾ƒå…¸å‹çš„ä¸€ä¸ªåœºæ™¯ï¼Œå› ä¸ºçœŸå®ä¸­ `global_vars_a *= 2` çš„è¿™ä¸ªæ“ä½œä¼šåˆ†ä¸ºä¸‰æ­¥ï¼Œä¸€æ­¥å…ˆä»å†…å­˜ä¸­è¯»å– `global_vars_a` åˆ°å¯„å­˜å™¨ä¸­ï¼Œç„¶ååœ¨å¯„å­˜å™¨å®Œæˆä¹˜æ³•æˆ–ä½è¿ç®—ï¼Œæœ€åå°†å¯„å­˜å™¨ä¸­ç®—å¥½çš„å€¼é‡æ–°å›å†™åˆ°å†…å­˜é‡Œã€‚

é‚£ä¹ˆæˆ‘ä»¬å‡è®¾è¿™ä¸ª spinlock æ²¡æœ‰ä»»ä½•å†…å­˜åºä¿æŠ¤ï¼Œå°±æœ‰å¯èƒ½å›å‡ºç° `global_vars_a` å…ˆäº `&lock` è¢«è¯»å–ï¼

```c
register a0 = READ_ONCE(global_vars_a);
spin_lock(&lock);
a0 *= 2;
```

è¿™å°±æ˜¯æ‰€è°“çš„ä¹±åºå‘ç”Ÿäº†ï¼Œè¿™æ˜¯ä¼šç›´æ¥å¯¼è‡´ data race çš„ï¼Œå› ä¸º spinlock æ²¡èƒ½å°†å˜é‡çš„è¯»å–ä¿æŠ¤åœ¨ä¸´ç•ŒåŒºå†…ï¼ˆè¯»å–å®Œé”çš„çŠ¶æ€åï¼‰ã€‚

å›å†™ä¹Ÿæ˜¯åŒç†ï¼Œå¦‚æœæ²¡æœ‰å†…å­˜åºä¿æŠ¤ï¼Œå°±ä¼šå‘ç”Ÿå¯èƒ½å·²ç» spin_unlock äº†ä¹‹å `global_vars_a` æ‰è¢«å†™å›å»çš„æƒ…å†µã€‚

# Store-Load

è¿™ä¸ªæˆ‘è§‰å¾—å…¶å®æ˜¯ä¸ªæœ‰è¶£çš„é—®é¢˜ï¼Œä¸ºä»€ä¹ˆå¦‚æ­¤å¸¸ç”¨çš„ acquire å’Œ release è¯­ä¹‰å±…ç„¶ä¸ä¿è¯ Store-Load å†…å­˜åºå‘¢ï¼Ÿ

è¿˜æ˜¯ [Preshing](https://preshing.com/20170612/can-reordering-of-release-acquire-operations-introduce-deadlock/) æŒ‡å‡ºäº†è¿™ä¸€ç‚¹ï¼Œæ–‡ä¸­çš„ä¸€ä¸ªä¾‹å­ï¼Œ

```cpp
A.store(1, std::memory_order_release);
int b = B.load(std::memory_order_acquire);
```

å› ä¸º Store-Load åºçš„ç¼ºå¤±ï¼Œæ‰€ä»¥å®é™…ä¸Š CPU å¯ä»¥æŠŠä»–é‡æ’æˆï¼ˆç­‰ä»·çš„è¯­è¨€æè¿°ï¼‰

```cpp
int b = B.load(std::memory_order_acquire);
A.store(1, std::memory_order_release);
```

è¿™é‡Œçš„ A å’Œ B ä¸¤ä¸ª atomic æ˜¯å¯èƒ½è¢«é‡æ’çš„ã€‚

å…¶å®å¦‚æœæ˜¯åŒä¸€ä¸ªå˜é‡ï¼Œæˆ–è€…è¯´å˜é‡ä¹‹é—´å­˜åœ¨æŒ‡é’ˆç­‰ä¾èµ–å…³ç³»æ—¶ï¼Œå¦‚ ARM64 è¿™ç§æ¶æ„å®é™…ä¸Šæ˜¯æœ‰ [data dependency](https://duetorun.com/blog/20231006/a64-memory-ordering/#secure_order) è¿™æ ·çš„æ¦‚å¿µçš„ï¼Œå®ƒä¿è¯äº†åŒä¸€ä¸ªå˜é‡ä¸‹çš„ Store-Load åºã€‚ä½†æ˜¯å¯¹äºä¸åŒçš„å˜é‡æ¥è¯´ï¼Œå®ƒå¹¶ä¸èƒ½å®Œå…¨ä¿è¯ã€‚

å¹¶ä¸”çš„å¹¶ä¸”ï¼ŒARM64 å®é™…ä¸Šæœ‰ `stlr` å’Œ `ldar` è¿™ä¸¤ä¸ªæŒ‡ä»¤ï¼Œå®ƒä»¬åœ¨ Acquire Release çš„åŸºç¡€ä¸Šä¹Ÿå¢åŠ äº†å¯¹ Store-Load åºçš„ä¸€äº›ä¿è¯ï¼Œç›¸å½“äºéƒ¨åˆ†çš„ SCï¼ˆAcquire å’Œ Release åˆ†åˆ«éƒ½å¤šäº†è¿™ä¸€ä¸ªå†…å­˜ç»­ä¿è¯ï¼‰ã€‚

å¦å¤–ä½œè€…æŒ‡å‡ºäº† C++ æ ‡å‡†ä¸­çš„ä¸€äº›è¯´æ˜ï¼Œ

> *An implementation should ensure that the last value (in modification order) assigned by an atomic or synchronization operation will become visible to all other threads in a finite period of time.*

æ¢è¨€ä¹‹å°±æ˜¯åªè¦æˆ‘ Store æŒ‡ä»¤å‘å‡ºåï¼Œé‚£ä¹ˆæ¶æ„å¿…é¡»è¦ä¿è¯åœ¨æœ‰é™çš„æ—¶é—´å†…å…¶ä»– Load æŒ‡ä»¤æœ€ç»ˆå¯ä»¥çœ‹åˆ°è¿™ä¸ªç»“æœï¼Œè¿™æ ·å°±é¿å…äº†å¾ˆå¤šå¯èƒ½å­˜åœ¨çš„é—®é¢˜ã€‚

```cpp
std::atomic<int> A = 0;
std::atomic<int> B = 0;

void thread1() {
    A.store(1, std::memory_order_release);

    while (B.load(std::memory_order_acquire) == 0) {
    }
}

void thread2() {
    while (A.load(std::memory_order_acquire) == 0) {
    }

    B.store(1, std::memory_order_release);
}
```

è¿™ä¸ªä¾‹å­é‡Œï¼Œå‡è®¾æ²¡æœ‰æ ‡å‡†çº¦æŸçš„è¯ï¼Œå®ƒæ˜¯å¯èƒ½ä¼šè¢«é‡æ’æˆ

```cpp
void thread1() {
    while (B.load(std::memory_order_acquire) == 0) {
    }

    A.store(1, std::memory_order_release);
}
```

è¿™æ ·å°±ä¼šé™·å…¥æ­»é”çš„é—®é¢˜ï¼Œä½†æ˜¯æœ‰äº†ä¸Šé¢æ ‡å‡†åï¼Œæ—¢ç„¶ç›®å‰ç¼–è¯‘å™¨ä¸ä¼šé‡æ’è¿™ä¸ªæŒ‡ä»¤ï¼Œé‚£ä¹ˆ `A.store` æŒ‡ä»¤ä¸€å®šæ˜¯å…ˆäº `B.load` æŒ‡ä»¤å°±å‘å‡ºäº†çš„ï¼Œè¿™é‡Œä¹Ÿå°±å¿…é¡»ä¿è¯æ— è®º `thread2` è‡ªæ—‹å¤šä¹…ï¼Œå®ƒæ€»èƒ½å› ä¸º `A.store` æ‰§è¡ŒæˆåŠŸè€Œæ‰§è¡Œåé¢çš„ `B.store` ï¼Œå¹¶ä¸”è¿™ä¸ªæ—¶é—´éœ€è¦æ˜¯â€œfiniteâ€çš„ã€‚

è¿™ä¸€æ¡ä¹Ÿå…¶å®å°±é™åˆ¶äº†å¤„ç†å™¨ï¼Œå³ä½¿ä½ ä¹±åºæ‰§è¡Œï¼Œä½†æ˜¯è¯¥æ‰§è¡Œçš„è¿˜æ˜¯å¾—æ‰§è¡Œã€‚

[More about ARM64](https://stackoverflow.com/a/67397890)ï¼š

> Yup, stlr is store-release on its own, and ldar can't pass an earlier stlr (i.e. no StoreLoad reordering) - that interaction between them satisfies that part of the seq_cst requirements which acq / rel doesn't have. ([ARMv8.3 ldapr](https://community.arm.com/groups/processors/blog/2016/10/27/armv8-a-architecture-2016-additions) is like ldar without that interaction, being only a plain acquire load, allowing more efficient acq_rel.)

ä¹Ÿå°±æ˜¯è¯´ `ldar` å’Œ `stlr` ä¹‹é—´æ˜¯è¢«æ’åºäº†çš„ï¼Œä¿è¯äº† Store-Release é¡ºåºã€‚

## LDAPR

```cpp
AccessDescriptor CreateAccDescLDAcqPC(boolean tagchecked)
  AccessDescriptor accdesc = NewAccDesc(AccessType_GPR);
  accdesc.acqpc         = TRUE;
  accdesc.read          = TRUE;
  accdesc.pan           = TRUE;
  accdesc.tagchecked    = tagchecked;
  accdesc.transactional = IsFeatureImplemented(FEAT_TME) && TSTATE.depth > 0;

  return accdesc;
```

## LDAR

```cpp
AccessDescriptor CreateAccDescAcqRel(MemOp memop, boolean tagchecked)
  AccessDescriptor accdesc = NewAccDesc(AccessType_GPR);
  accdesc.acqsc         = memop == MemOp_LOAD;
  accdesc.relsc         = memop == MemOp_STORE;
  accdesc.read          = memop == MemOp_LOAD;
  accdesc.write         = memop == MemOp_STORE;
  accdesc.pan           = TRUE;
  accdesc.tagchecked    = tagchecked;
  accdesc.transactional = IsFeatureImplemented(FEAT_TME) && TSTATE.depth > 0;
  
  return accdesc;
```

å¯ä»¥çœ‹åˆ° `ldar` æœç„¶å·²ç»å« `acqsc` å’Œ `relsc` äº†ï¼Œä¹Ÿå°±æ˜¯ SC Acqure/Release äº†å§ï¼ˆã€‚

è¿˜å¯ä»¥ç»§ç»­çœ‹ä¸€ä¸‹ `AccessDescriptor` çš„ç›¸å…³å®šä¹‰ï¼Œ

```cpp
boolean acqsc,          // Acquire with Sequential Consistency
boolean acqpc,          // FEAT_LRCPC: Acquire with Processor Consistency
boolean relsc,          // Release with Sequential Consistency
boolean limitedordered, // FEAT_LOR: Acquire/Release with limited ordering
```

CPUï¼ŒçœŸç¥å¥‡ã€‚

# Power-of-2

<https://max0x7ba.github.io/atomic_queue/#ring-buffer-capacity>

> The writer and reader indexes get mapped into the ring-buffer array index using remainder binary operator % SIZE. Remainder binary operator % normally generates a division CPU instruction which isnâ€™t cheap, but using a power-of-2 size turns that remainder operator into one cheap binary and CPU instruction and that is as fast as it gets.

ä»¥åŠ

> The element index within the cache line gets swapped with the cache line index, so that consecutive queue elements reside in different cache lines. This massively reduces cache line contention between multiple producers and multiple consumers. Instead of N producers together with M consumers competing on subsequent elements in the same ring-buffer cache line in the worst case, it is only one producer competing with one consumer (pedantically, when the number of CPUs is not greater than the number of elements that can fit in one cache line). This optimisation scales better with the number of producers and consumers, and element size. With low number of producers and consumers (up to about 2 of each in these benchmarks) disabling this optimisation may yield better throughput (but higher variance across runs).

å½“ç„¶è¿™ä¸ªåªæ˜¯æ¯”è¾ƒâ€œperformanceâ€çš„åŸå› ã€‚
